# LDA yet? Transformer is All You Need â€” Mini-LLM  
*26 M-parameter decoder-only model Â· NeurIPS LDA analysis*

> A compact Transformer that still talks like GPT-style models â€” and the journey from traditional **LDA** topic modelling to end-to-end **LLM** training.  
> æœ¬ä»“åº“åŒ…å«å®Œæ•´ä»£ç ã€æ•°æ®å¤„ç†è„šæœ¬ä¸æ£€æŸ¥ç‚¹ï¼Œå¸®åŠ©ä½ ä»é›¶å¤ç°æŠ¥å‘Šä¸­çš„å…¨éƒ¨å®éªŒã€‚

---
![æ¨¡å‹ç»“æ„å›¾](arch.png)
## ä»»åŠ¡å—

| æ¨¡å—             | å…³é”®ç‰¹æ€§                                                                 |
|------------------|--------------------------------------------------------------------------|
| **LDA åˆ†æ**     | Â· é’ˆå¯¹ **1 000** ç¯‡ NeurIPS è®ºæ–‡æŠ½å– **10** ä¸ªä¸»é¢˜<br>Â· Gibbs é‡‡æ ·æ¨æ–­ + å…³é”®è¯å¯è§†åŒ– |
| **Mini-LLM**     | Â· **26 M** å‚æ•°ï¼ŒDecoder-Only æ¶æ„<br>Â· **RoPE** ä½ç½®ç¼–ç ï¼Œæ”¯æŒè¶…é•¿ä¸Šä¸‹æ–‡<br>Â· **RMSNorm + SwiGLU + GQA**ï¼Œæ˜¾å­˜ â†“ 40%ï¼Œæ¨ç† â†‘ 1.3â€“1.6Ã— |
| **ç«¯åˆ°ç«¯æµç¨‹**   | Â· æ•°æ®æŠ“å– â†’ æ¸…æ´— â†’ é¢„è®­ç»ƒ â†’ å¾®è°ƒ â†’ æ¨ç†<br>Â· é¢„è®­ç»ƒå›°æƒ‘åº¦ **1600 â†’ 12**ï¼Œå¾®è°ƒå **10 â†’ 5.5** |

---

## ğŸ“‚ é¡¹ç›®ç»“æ„

```
â”œâ”€â”€ data/                 # åŸå§‹ä¸å¤„ç†åæ•°æ®
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ model/            # Transformer æ¶æ„å®šä¹‰
â”‚   â”œâ”€â”€ train.py          # è®­ç»ƒä¸»ç¨‹åºï¼ˆé¢„è®­ç»ƒä¸å¾®è°ƒï¼‰
â”‚   â””â”€â”€ utils/            # å·¥å…·å‡½æ•°ï¼ˆRoPE, è¯„ä¼°ç­‰ï¼‰
â”œâ”€â”€ checkpoints/          # æ¨¡å‹æ£€æŸ¥ç‚¹ï¼ˆé¢„è®­ç»ƒä¸å¾®è°ƒç»“æœï¼‰
â””â”€â”€ notebooks/            # LDA åˆ†æç¬”è®°æœ¬
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. åˆ›å»ºç¯å¢ƒ
```bash
pip install -r requirements.txt
```

### 2. ä¸‹è½½/å‡†å¤‡æ•°æ®
> ç”±äºè®­ç»ƒæ•°æ®é›†è¿‡å¤§ï¼Œéœ€è¦è¯·è¯·é‚®ä»¶å’¨è¯¢ï¼šloyiv5477@gmail.com

### 3. å¯åŠ¨é¢„è®­ç»ƒ
```bash
python src/pre-training.py \
  --config configs/pretrain.yaml
```

### 4. è¿›è¡Œç›‘ç£å¾®è°ƒ
```bash
python src/full-SFT-train.py \
  --stage sft \
  --config configs/sft.yaml \
  --ckpt checkpoints/pretrain.pt
```

### 5. æ¨ç†ç¤ºä¾‹
```python
from src.model import MiniLLM
model = MiniLLM.load_from_checkpoint("checkpoints/sft.pt").eval().cuda()
print(model.chat("è®²è®² LDA ä¸ Transformer çš„å·®å¼‚ï¼Ÿ"))
```

---

## ğŸ“Š å®éªŒç»“æœ

| é˜¶æ®µ     | è®­ç»ƒæ­¥æ•°    | äº¤å‰ç†µ Loss â†“        | å›°æƒ‘åº¦ â†“          |
|----------|-------------|----------------------|-------------------|
| é¢„è®­ç»ƒ   | 0 â†’ 1 300   | 7.4 â†’ **2.5 Â± 0.15** | 1600 â†’ **12.2**   |
| å¾®è°ƒ SFT | 0 â†’ 4 500   | 2.3 â†’ **1.70 Â± 0.15**| 10.0 â†’ **5.5**    |

---

## ğŸ“ˆ LDA ä¸»é¢˜åˆ†å¸ƒï¼ˆNeurIPSï¼‰

<details>
<summary>ç‚¹å‡»å±•å¼€æŸ¥çœ‹ 10 ä¸ªä¸»é¢˜å…³é”®è¯</summary>

- **Topic 0**â€ƒModel construction & training  
- **Topic 1**â€ƒData-driven methods  
- **Topic 2**â€ƒOptimization algorithms  
- **Topic 3**â€ƒDeep architectures  
- **Topic 4**â€ƒBayesian methods  
- **Topic 5**â€ƒReinforcement learning  
- **Topic 6**â€ƒNatural language processing  
- **Topic 7**â€ƒGenerative models  
- **Topic 8**â€ƒGraph & structured data  
- **Topic 9**â€ƒVisual / speech recognition  

</details>

---

## ğŸ“ å¼•ç”¨å‚è€ƒ

å¦‚æœæœ¬é¡¹ç›®å¯¹ä½ çš„å·¥ä½œæœ‰å¸®åŠ©ï¼Œæ¬¢è¿å¼•ç”¨ï¼š

```bibtex
@report{liu2025lda_transformer,
  title  = {{LDA yet? Transformer is All You Need}},
  author = {Youwei Liu and Wen Shi},
  year   = {2025},
  note   = {Central South University, Technical Report},
  url    = {https://github.com/loyiv/mini-LLM}
}
```

---




## ğŸ™ è‡´è°¢

æœ¬é¡¹ç›®ç”±ä¸­å—å¤§å­¦åˆ˜åˆç®è‡ªä¸»å®Œæˆï¼Œæ„Ÿè°¢æ–½æ–‡æ•™æˆã€Šæ–‡æœ¬åˆ†æä¸æ–‡æœ¬æŒ–æ˜ã€‹è¯¾ç¨‹çš„æ‚‰å¿ƒæŒ‡å¯¼ä¸å¯å‘ã€‚
